# -*- coding: utf-8 -*-
"""Portfolio Project-RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xF7gtZQoMcL8qArnp3BNZpoT3HyWShGS
"""

#===========================
#Dr. Ray Islam
#=============================

#  Clean conflicting libs (safe) + install minimal deps ---
!pip -q uninstall -y transformers accelerate peft 2>/dev/null
!pip -q install -U pypdf faiss-cpu sentence-transformers llama-cpp-python huggingface_hub

import os, re, textwrap
import numpy as np
from dataclasses import dataclass
from typing import List, Tuple, Optional

from pypdf import PdfReader
import faiss
from sentence_transformers import SentenceTransformer
from huggingface_hub import hf_hub_download, list_repo_files
from llama_cpp import Llama

# -----------------------
# 1) Select your PDF
# -----------------------
pdfs = sorted([f for f in os.listdir("/content") if f.lower().endswith(".pdf")])
if not pdfs:
    raise FileNotFoundError("No PDFs found in /content. Upload a PDF to Colab first.")

print("PDFs found in /content:")
for i, f in enumerate(pdfs):
    print(f"[{i}] {f}")

PDF_INDEX = 0          # <-- CHANGE THIS to pick a different PDF
MAX_PAGES = None       # <-- Optional speed-up: set e.g. 60 or 100 for fast demo builds

PDF_FILE = f"/content/{pdfs[PDF_INDEX]}"
DOC_ID = os.path.basename(PDF_FILE)
print("\nUsing PDF:", PDF_FILE)

# -----------------------
# 2) Extract + chunk text
# -----------------------
@dataclass
class Chunk:
    doc_id: str
    page: int
    text: str

def clean_text(t: str) -> str:
    t = t.replace("\x00", " ")
    t = re.sub(r"\s+", " ", t).strip()
    return t

def extract_pdf_text(pdf_path: str, max_pages: Optional[int] = None) -> List[Tuple[int, str]]:
    reader = PdfReader(pdf_path)
    n = len(reader.pages) if max_pages is None else min(len(reader.pages), max_pages)
    pages = []
    for i in range(n):
        txt = reader.pages[i].extract_text() or ""
        txt = clean_text(txt)
        if txt:
            pages.append((i + 1, txt))  # 1-indexed pages
    return pages

def chunk_pages(pages: List[Tuple[int, str]], doc_id: str, chunk_size: int = 900, overlap: int = 150) -> List[Chunk]:
    chunks: List[Chunk] = []
    for page_num, text in pages:
        start = 0
        while start < len(text):
            end = min(len(text), start + chunk_size)
            chunk = clean_text(text[start:end])
            if len(chunk) >= 200:
                chunks.append(Chunk(doc_id=doc_id, page=page_num, text=chunk))
            if end >= len(text):
                break
            start = max(0, end - overlap)
    return chunks

print("\nExtracting text...")
pages = extract_pdf_text(PDF_FILE, max_pages=MAX_PAGES)
print(f"Extracted {len(pages)} text pages")

print("Chunking...")
chunks = chunk_pages(pages, doc_id=DOC_ID, chunk_size=900, overlap=150)
print(f"Created {len(chunks)} chunks")

# -----------------------
# 3) Embed + build FAISS index
# -----------------------
print("\nEmbedding + indexing (FAISS)...")
emb_model_name = "sentence-transformers/all-MiniLM-L6-v2"
emb_model = SentenceTransformer(emb_model_name)

embeddings = emb_model.encode([c.text for c in chunks], normalize_embeddings=True, show_progress_bar=True)
embeddings = np.asarray(embeddings, dtype=np.float32)

index = faiss.IndexFlatIP(embeddings.shape[1])  # cosine sim via normalized vectors + inner product
index.add(embeddings)

print("âœ… Vector index ready.")
print("Embedding dim:", embeddings.shape[1])

def retrieve(query: str, top_k: int = 6):
    q = emb_model.encode([query], normalize_embeddings=True)
    q = np.asarray(q, dtype=np.float32)
    scores, idx = index.search(q, top_k)
    results = []
    for s, i in zip(scores[0], idx[0]):
        if i == -1:
            continue
        results.append((float(s), chunks[int(i)]))
    return results

def build_context(results, max_chars: int = 3500) -> str:
    out, used = [], 0
    for score, ch in results:
        block = f"[Source: {ch.doc_id} | page {ch.page} | score {score:.3f}]\n{ch.text}\n"
        if used + len(block) > max_chars and out:
            break
        out.append(block)
        used += len(block)
    return "\n".join(out)

# -----------------------
# 4) Download + load open-source LLM (GGUF) with llama.cpp
# -----------------------

print("\nDownloading open-source LLM (GGUF) from Hugging Face...")
repo_id = "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"

# Robust file selection (avoids 404s if naming differs)
repo_files = list_repo_files(repo_id)
preferred_files = [
    "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",  # good speed/quality
    "tinyllama-1.1b-chat-v1.0.Q4_K_S.gguf",
    "tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf",
]
gguf_name = next((f for f in preferred_files if f in repo_files), None)
if gguf_name is None:
    gguf_name = next((f for f in repo_files if f.lower().endswith(".gguf")), None)
if gguf_name is None:
    raise RuntimeError("No .gguf file found in the model repo. Repo structure may have changed.")

gguf_path = hf_hub_download(repo_id=repo_id, filename=gguf_name)
print("Using GGUF:", gguf_name)
print("Local path:", gguf_path)

# Load llama.cpp model (CPU-friendly)
llm = Llama(
    model_path=gguf_path,
    n_ctx=2048,
    n_threads=2,     # raise to 4 if you want
    verbose=False
)

def generate_answer(question: str, top_k: int = 6):
    results = retrieve(question, top_k=top_k)
    context = build_context(results)

    prompt = f"""You are a helpful assistant.
Use ONLY the provided context from the document to answer the question.
If the context is insufficient, say: "I don't know based on the document."

Question:
{question}

Context:
{context}

Answer (include citations like "(page 25)"):"""

    out = llm(
        prompt,
        max_tokens=280,
        temperature=0.2,
        top_p=0.9
    )
    answer = out["choices"][0]["text"].strip()
    evidence = sorted({(ch.doc_id, ch.page) for _, ch in results})
    return answer, evidence, results

def ask(question: str, top_k: int = 8, show_passages: bool = True, preview_chars: int = 900):
    answer, evidence, results = generate_answer(question, top_k=top_k)

    print("="*110)
    print("QUESTION:", question)
    print("="*110)
    print("\nANSWER:\n", answer)

    print("\nEVIDENCE (doc, page):", evidence)

    if show_passages:
        print("\nTOP PASSAGES:")
        for score, ch in results:
            print(f"\n--- {ch.doc_id} | page {ch.page} | score={score:.3f} ---")
            print(textwrap.fill(ch.text[:preview_chars], width=110))
    print("="*110)
    return answer, evidence, results

# Example questions (edit these):
ask("Find the section that describes the purpose of the Annual Statistical Supplement 2016.", top_k=10)