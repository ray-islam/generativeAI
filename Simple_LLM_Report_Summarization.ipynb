{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGhzRbjwEbcUKDW/Np6tXl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ray-islam/generativeAI/blob/main/Simple_LLM_Report_Summarization-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pUhxn3nCrqM"
      },
      "outputs": [],
      "source": [
        "#Simple LLM Report Summarization\n",
        "\n",
        "# Hard reset the broken wheels (DO THIS FIRST)\n",
        "!pip -q uninstall -y numpy\n",
        "!pip -q uninstall -y transformers tokenizers safetensors accelerate\n",
        "!pip -q uninstall -y scipy scikit-learn pandas\n",
        "!pip -q cache purge\n",
        "\n",
        "# Install a compatible stack for Colab\n",
        "!pip -q install --no-cache-dir \"numpy==2.1.3\"\n",
        "!pip -q install --no-cache-dir \"transformers==4.44.2\" \"tokenizers==0.19.1\" \"accelerate==0.33.0\" \"safetensors>=0.4.3\" \"pypdf==4.2.0\"\n",
        "\n",
        "# Restart runtime (MANDATORY)\n",
        "import os, sys\n",
        "os.kill(os.getpid(), 9)\n",
        "\n",
        "# Sanity check (must pass before any summarization code)\n",
        "import numpy as np\n",
        "import transformers\n",
        "import tokenizers\n",
        "import accelerate\n",
        "\n",
        "print(\"numpy:\", np.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"tokenizers:\", tokenizers.__version__)\n",
        "print(\"accelerate:\", accelerate.__version__)\n",
        "\n",
        "#Imports + File picker from /content\n",
        "import re, time, textwrap\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from pypdf import PdfReader\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = s.replace(\"\\x00\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def read_pdf_text(pdf_path: str) -> str:\n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = []\n",
        "    for p in reader.pages:\n",
        "        pages.append(p.extract_text() or \"\")\n",
        "    return \"\\n\".join(pages)\n",
        "\n",
        "def read_txt_text(txt_path: str) -> str:\n",
        "    return Path(txt_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "def find_files(base_dir=\"/content\", exts=(\".pdf\", \".txt\")):\n",
        "    base = Path(base_dir)\n",
        "    found = []\n",
        "    for ext in exts:\n",
        "        found.extend(base.rglob(f\"*{ext}\"))\n",
        "    return sorted(found)\n",
        "\n",
        "def choose_file_interactive(base_dir=\"/content\"):\n",
        "    files = find_files(base_dir=base_dir, exts=(\".pdf\", \".txt\"))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No .pdf/.txt found under {base_dir}. Upload in the Files panel.\")\n",
        "    print(\"Found files:\\n\")\n",
        "    for i, p in enumerate(files, 1):\n",
        "        print(f\"{i:>2}. {p}\")\n",
        "    idx = int(input(\"\\nEnter file number to summarize: \").strip())\n",
        "    if idx < 1 or idx > len(files):\n",
        "        raise ValueError(\"Invalid selection.\")\n",
        "    return str(files[idx-1])\n",
        "\n",
        "# Build summarizer\n",
        "MODEL_ID = \"facebook/bart-large-cnn\"\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=MODEL_ID,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Token-aware chunking + summarization\n",
        "def chunk_by_tokens(text: str, tokenizer, max_input_tokens: int = 900, overlap_tokens: int = 60):\n",
        "    text = clean_text(text)\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(ids):\n",
        "        end = min(start + max_input_tokens, len(ids))\n",
        "        chunk_ids = ids[start:end]\n",
        "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
        "        chunks.append(chunk_text.strip())\n",
        "\n",
        "        if end == len(ids):\n",
        "            break\n",
        "        start = max(0, end - overlap_tokens)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def summarize_chunks(summarizer, chunks, max_len=160, min_len=60, sleep_s=0.0):\n",
        "    out = []\n",
        "    n = len(chunks)\n",
        "    for i, c in enumerate(chunks, 1):\n",
        "        if len(c) < 200:\n",
        "            continue\n",
        "        res = summarizer(\n",
        "            c,\n",
        "            max_length=max_len,\n",
        "            min_length=min_len,\n",
        "            do_sample=False,\n",
        "            truncation=True\n",
        "        )[0][\"summary_text\"]\n",
        "        out.append(res.strip())\n",
        "        print(f\"âœ“ Chunk {i}/{n} summarized.\")\n",
        "        if sleep_s:\n",
        "            time.sleep(sleep_s)\n",
        "    return out\n",
        "\n",
        "def final_compress(summarizer, text: str, tokenizer, max_input_tokens=700):\n",
        "    text = clean_text(text)\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    chunks = chunk_by_tokens(text, tokenizer, max_input_tokens=max_input_tokens, overlap_tokens=40)\n",
        "    finals = summarize_chunks(summarizer, chunks, max_len=200, min_len=80)\n",
        "    return clean_text(\" \".join(finals))\n",
        "\n",
        "# Run end-to-end + save/download\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "path = choose_file_interactive(\"/content\")\n",
        "print(\"\\nSelected:\", path)\n",
        "\n",
        "if path.lower().endswith(\".pdf\"):\n",
        "    raw_text = read_pdf_text(path)\n",
        "elif path.lower().endswith(\".txt\"):\n",
        "    raw_text = read_txt_text(path)\n",
        "else:\n",
        "    raise ValueError(\"Pick a .pdf or .txt\")\n",
        "\n",
        "raw_text = clean_text(raw_text)\n",
        "print(f\"\\nLoaded {len(raw_text):,} characters.\")\n",
        "print(raw_text[:800] + (\"...\" if len(raw_text) > 800 else \"\"))\n",
        "\n",
        "CHUNK_INPUT_TOKENS = 900\n",
        "chunks = chunk_by_tokens(raw_text, tokenizer, max_input_tokens=CHUNK_INPUT_TOKENS, overlap_tokens=60)\n",
        "print(\"\\nTotal chunks:\", len(chunks))\n",
        "\n",
        "chunk_summaries = summarize_chunks(summarizer, chunks, max_len=160, min_len=60)\n",
        "\n",
        "intermediate = \"\\n\\n\".join(chunk_summaries)\n",
        "print(\"\\n--- Intermediate (first 800 chars) ---\\n\")\n",
        "print(intermediate[:800] + (\"...\" if len(intermediate) > 800 else \"\"))\n",
        "\n",
        "final_summary = final_compress(summarizer, intermediate, tokenizer, max_input_tokens=700)\n",
        "\n",
        "print(\"\\n======= EXECUTIVE SUMMARY =======\\n\")\n",
        "print(textwrap.fill(final_summary, 100))\n",
        "\n",
        "prompt = (\n",
        "    \"HEADLINE AND BULLETS:\\n\"\n",
        "    + final_summary\n",
        "    + \"\\n\\nWrite a short headline and 5 bullet points with key findings.\"\n",
        ")\n",
        "headline_bullets = summarizer(prompt, max_length=220, min_length=120, do_sample=False, truncation=True)[0][\"summary_text\"]\n",
        "\n",
        "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "base = Path(path).stem\n",
        "\n",
        "out_exec = f\"{base}_executive_summary_{stamp}.txt\"\n",
        "out_chunks = f\"{base}_chunk_summaries_{stamp}.txt\"\n",
        "out_hb = f\"{base}_headline_bullets_{stamp}.txt\"\n",
        "\n",
        "Path(out_exec).write_text(final_summary, encoding=\"utf-8\")\n",
        "Path(out_chunks).write_text(intermediate, encoding=\"utf-8\")\n",
        "Path(out_hb).write_text(headline_bullets, encoding=\"utf-8\")\n",
        "\n",
        "files.download(out_exec)\n",
        "files.download(out_chunks)\n",
        "files.download(out_hb)\n",
        "\n"
      ]
    }
  ]
}
